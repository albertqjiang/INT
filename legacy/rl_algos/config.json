{
  "supervised_baseline": {
    "objective_path": "../../data/objectives_by_degree/simple_proof_degree_{}_{}_objectives.p",
    "simple_proof_path": "../../data/simple_proof.p",
    "saving_dir": "../../pt_models/imitation_aided/",
    "pretrain": "False",
    "total_random_seeds": 1,
    "gt_maxsize": 10,
    "ent_maxsize": 15,
    "lemma_maxsize": 25,
    "lemma_operand_maxsize": 4,
    "obj_maxsize": 1,
    "total_degrees": 3,
    "objectives_per_degree": 10,
    "total_updates_per_degree": 20000,
    "max_step": 200,
    "training_scheme": [
      "attentive"
    ],
    "higher_hidden_dim": 128,
    "lower_hidden_dim": 128,
    "higher_lr": 1e-2,
    "lower_lr": 3e-4,
    "encoder_lr": 3e-4,
    "higher_epochs": 1,
    "lower_epochs": 1,
    "batch_size": 32,
    "discount_factor": 0.99,
    "lower_reward_scheme": {
      "REWARD_NULL": 0,
      "REWARD_INPUT_INVALID": -1,
      "REWARD_ASSUMPTION_INVALID": -1,
      "REWARD_DUPLICATED_RESULTS": -1,
      "REWARD_THEOREM_PROCEEDED": 1,
      "REWARD_PROOF_COMPLETE": 100
    },
    "higher_reward_scheme": {
      "REWARD_NULL": 0,
      "REWARD_INPUT_INVALID": -1,
      "REWARD_ASSUMPTION_INVALID": -1,
      "REWARD_DUPLICATED_RESULTS": -1,
      "REWARD_THEOREM_PROCEEDED": 1,
      "REWARD_PROOF_COMPLETE": 1000
    },
    "imitation_reward": 100
  },
  "batch_encode": {
    "entity_embedding_capacity": 5,
    "entity_embedding_size": 128,
    "entity_maxsize": 20,
    "ground_truth_maxsize": 20,
    "theorem_maxsize": 30,
    "objective_maxsize": 1,
    "theorem_operand_maxsize": 5
  },
  "supervised_her": {
    "objective_path": "../../data/objectives_by_degree/simple_proof_degree_{}_{}_objectives.p",
    "simple_proof_path": "../../data/simple_proof.p",
    "saving_dir": "../../pt_models/supervised_her/",
    "supervised_model_dir": "../../best_model/imitation_learning/model.pt",
    "pretrain": "True",
    "total_random_seeds": 1,
    "number_of_configurations": 20,
    "gt_maxsize": 10,
    "ent_maxsize": 15,
    "lemma_maxsize": 25,
    "lemma_operand_maxsize": 4,
    "obj_maxsize": 1,
    "total_degrees": 10,
    "total_updates_per_degree": 10000,
    "objectives_per_degree": 100,
    "max_step": 100,
    "training_scheme": [
      "attentive"
    ],
    "entity_embedding_size": 128,
    "logic_statement_embedding_size": 128,
    "theorem_embedding_size": 128,
    "higher_hidden_dim": 128,
    "lower_hidden_dim": 128,
    "higher_lr": 3e-5,
    "lower_lr": 3e-5,
    "encoder_lr": 3e-5,
    "higher_epochs": 1,
    "lower_epochs": 1,
    "batch_size": 32,
    "higher_buffer_capacity": 7000,
    "lower_buffer_capacity": 7000,
    "discount_factor": 0.99,
    "lower_reward_scheme": {
      "REWARD_NULL": 0,
      "REWARD_INPUT_INVALID": -1,
      "REWARD_ASSUMPTION_INVALID": -1,
      "REWARD_DUPLICATED_RESULTS": -1,
      "REWARD_THEOREM_PROCEEDED": 1,
      "REWARD_PROOF_COMPLETE": 1000
    },
    "higher_reward_scheme": {
      "REWARD_NULL": 0,
      "REWARD_INPUT_INVALID": -1,
      "REWARD_ASSUMPTION_INVALID": -1,
      "REWARD_DUPLICATED_RESULTS": -1,
      "REWARD_THEOREM_PROCEEDED": 1,
      "REWARD_PROOF_COMPLETE": 1000
    }
  },
  "naive_her": {
    "objective_path": "../../data/objectives_by_degree/simple_proof_degree_{}_{}_objectives.p",
    "simple_proof_path": "../../data/simple_proof.p",
    "pretrained_model_path": "../../best_model/imitation_learning/model.pt",
    "saving_dir": "../../pt_models/naive_her/",
    "pretrain": "False",
    "total_random_seeds": 1,
    "number_of_configurations": 20,
    "gt_maxsize": 10,
    "ent_maxsize": 15,
    "lemma_maxsize": 25,
    "lemma_operand_maxsize": 4,
    "obj_maxsize": 1,
    "total_degrees": 10,
    "total_updates_per_degree": 10000,
    "objectives_per_degree": 100,
    "max_step": 100,
    "training_scheme": [
      "attentive"
    ],
    "entity_embedding_size": 128,
    "logic_statement_embedding_size": 128,
    "theorem_embedding_size": 128,
    "higher_hidden_dim": 128,
    "lower_hidden_dim": 128,
    "higher_lr": 3e-5,
    "lower_lr": 3e-5,
    "encoder_lr": 3e-5,
    "higher_epochs": 1,
    "lower_epochs": 1,
    "batch_size": 32,
    "higher_buffer_capacity": 7000,
    "lower_buffer_capacity": 7000,
    "discount_factor": 0.99,
    "lower_reward_scheme": {
      "REWARD_NULL": 0,
      "REWARD_INPUT_INVALID": -1,
      "REWARD_ASSUMPTION_INVALID": -1,
      "REWARD_DUPLICATED_RESULTS": -1,
      "REWARD_THEOREM_PROCEEDED": 1,
      "REWARD_PROOF_COMPLETE": 100
    },
    "higher_reward_scheme": {
      "REWARD_NULL": 0,
      "REWARD_INPUT_INVALID": -1,
      "REWARD_ASSUMPTION_INVALID": -1,
      "REWARD_DUPLICATED_RESULTS": -1,
      "REWARD_THEOREM_PROCEEDED": 1,
      "REWARD_PROOF_COMPLETE": 1000
    }
  },
  "imitation_aided_cts": {
    "proof_template_path": "../../data/cts/proof_templates.p",
    "simple_proof_path": "../../data/simple_proof.p",
    "saving_dir": "../../pt_models/imitation_aided_cts/",
    "pretrain": "False",
    "total_random_seeds": 1,
    "gt_maxsize": 10,
    "ent_maxsize": 15,
    "lemma_maxsize": 25,
    "lemma_operand_maxsize": 4,
    "obj_maxsize": 1,
    "total_degrees": 1,
    "total_updates_per_degree": 20000,
    "max_step": 200,
    "training_scheme": [
      "attentive"
    ],
    "higher_hidden_dim": 128,
    "lower_hidden_dim": 128,
    "higher_lr": 3e-5,
    "lower_lr": 3e-5,
    "encoder_lr": 3e-5,
    "higher_epochs": 1,
    "lower_epochs": 1,
    "batch_size": 32,
    "higher_buffer_capacity": 1024,
    "lower_buffer_capacity": 1024,
    "success_buffer_capacity": 1024,
    "discount_factor": 0.99,
    "lower_reward_scheme": {
      "REWARD_NULL": 0,
      "REWARD_INPUT_INVALID": -1,
      "REWARD_ASSUMPTION_INVALID": -1,
      "REWARD_DUPLICATED_RESULTS": -1,
      "REWARD_THEOREM_PROCEEDED": 1,
      "REWARD_PROOF_COMPLETE": 100
    },
    "higher_reward_scheme": {
      "REWARD_NULL": 0,
      "REWARD_INPUT_INVALID": -1,
      "REWARD_ASSUMPTION_INVALID": -1,
      "REWARD_DUPLICATED_RESULTS": -1,
      "REWARD_THEOREM_PROCEEDED": 1,
      "REWARD_PROOF_COMPLETE": 1000
    },
    "imitation_reward": 100
  },
  "imitation_aided": {
    "objective_path": "../../data/objectives_by_degree/simple_proof_degree_{}_{}_objectives.p",
    "simple_proof_path": "../../data/simple_proof.p",
    "saving_dir": "../../pt_models/imitation_aided/",
    "pretrain": "False",
    "total_random_seeds": 1,
    "gt_maxsize": 10,
    "ent_maxsize": 15,
    "lemma_maxsize": 25,
    "lemma_operand_maxsize": 4,
    "obj_maxsize": 1,
    "total_degrees": 3,
    "objectives_per_degree": 10,
    "total_updates_per_degree": 20000,
    "max_step": 200,
    "training_scheme": [
      "attentive"
    ],
    "higher_hidden_dim": 128,
    "lower_hidden_dim": 128,
    "higher_lr": 3e-5,
    "lower_lr": 3e-5,
    "encoder_lr": 3e-5,
    "higher_epochs": 1,
    "lower_epochs": 1,
    "batch_size": 128,
    "higher_buffer_capacity": 1024,
    "lower_buffer_capacity": 1024,
    "success_buffer_capacity": 1024,
    "discount_factor": 0.99,
    "lower_reward_scheme": {
      "REWARD_NULL": 0,
      "REWARD_INPUT_INVALID": -1,
      "REWARD_ASSUMPTION_INVALID": -1,
      "REWARD_DUPLICATED_RESULTS": -1,
      "REWARD_THEOREM_PROCEEDED": 1,
      "REWARD_PROOF_COMPLETE": 100
    },
    "higher_reward_scheme": {
      "REWARD_NULL": 0,
      "REWARD_INPUT_INVALID": -1,
      "REWARD_ASSUMPTION_INVALID": -1,
      "REWARD_DUPLICATED_RESULTS": -1,
      "REWARD_THEOREM_PROCEEDED": 1,
      "REWARD_PROOF_COMPLETE": 1000
    },
    "imitation_reward": 100
  },
  "hier_q": {
    "encoder_path": "../pt_models/logic_recur_nn/logic_recursive_nn.pt",
    "higher_lr": 3e-4,
    "lower_lr": 3e-4,
    "replay_buffer_capacity": 4096,
    "batch_size": 128,
    "max_frames": 100000,
    "max_steps": 500,
    "saving_dir": "../pt_models/hier_q/",
    "saving_affix": "",
    "discount_factor": 0.9,
    "lower_reward_dict": {
      "REWARD_NULL": 0,
      "REWARD_INPUT_INVALID": -1,
      "REWARD_ASSUMPTION_INVALID": -1,
      "REWARD_DUPLICATED_RESULTS": -1,
      "REWARD_THEOREM_PROCEEDED": 1,
      "REWARD_PROOF_COMPLETE": 1
    },
    "higher_reward_dict": {
      "REWARD_NULL": 0,
      "REWARD_INPUT_INVALID": -1,
      "REWARD_ASSUMPTION_INVALID": -1,
      "REWARD_DUPLICATED_RESULTS": -1,
      "REWARD_THEOREM_PROCEEDED": 1,
      "REWARD_PROOF_COMPLETE": 1000
    }
  },
  "hier_q_curriculum": {
    "pretrain": "False",
    "encoder_path": "../pt_models/logic_recur_nn/logic_recursive_nn.pt",
    "objective_format": "../data/objectives_by_degree/simple_proof_degree_{}_{}_objectives.p",
    "saving_dir": "../pt_models/hier_q_curriculum/",
    "simple_proof_path": "../data/simple_proof.p",
    "higher_lr": 3e-5,
    "lower_lr": 3e-5,
    "replay_buffer_capacity": 8192,
    "batch_size": 256,
    "max_frames": 1000,
    "max_steps": 100,
    "number_of_objectives": 10,
    "saving_affix": "",
    "train_val_split": 0.9,
    "discount_factor": 0.9,
    "max_degree": 10,
    "lower_reward_dict": {
      "REWARD_NULL": 0,
      "REWARD_INPUT_INVALID": -1,
      "REWARD_ASSUMPTION_INVALID": -1,
      "REWARD_DUPLICATED_RESULTS": -1,
      "REWARD_THEOREM_PROCEEDED": 1,
      "REWARD_PROOF_COMPLETE": 1
    },
    "higher_reward_dict": {
      "REWARD_NULL": 0,
      "REWARD_INPUT_INVALID": -1,
      "REWARD_ASSUMPTION_INVALID": -1,
      "REWARD_DUPLICATED_RESULTS": -1,
      "REWARD_THEOREM_PROCEEDED": 1,
      "REWARD_PROOF_COMPLETE": 1000
    }
  },
  "multi_step_q": {
    "objective_path": "../../data/objectives_by_degree/simple_proof_degree_{}_{}_objectives.p",
    "simple_proof_path": "../../data/simple_proof.p",
    "saving_dir": "../../pt_models/hier_q_curriculum_improved/",
    "pretrain": "False",
    "total_random_seeds": 1,
    "gt_maxsize": 50,
    "ent_maxsize": 30,
    "lemma_maxsize": 50,
    "lemma_operand_maxsize": 4,
    "obj_maxsize": 1,
    "total_degrees": 3,
    "objectives_per_degree": 10,
    "total_updates_per_degree": 80,
    "max_step": 200,
    "training_scheme": [
      "attentive"
    ],
    "higher_hidden_dim": 128,
    "lower_hidden_dim": 128,
    "higher_lr": 3e-5,
    "lower_lr": 3e-5,
    "encoder_lr": 3e-5,
    "higher_epochs": 1,
    "lower_epochs": 1,
    "batch_size": 32,
    "higher_buffer_capacity": 8192,
    "lower_buffer_capacity": 8192,
    "success_buffer_capacity": 1024,
    "discount_factor": 0.99,
    "lower_reward_scheme": {
      "REWARD_NULL": 0,
      "REWARD_INPUT_INVALID": -1,
      "REWARD_ASSUMPTION_INVALID": -1,
      "REWARD_DUPLICATED_RESULTS": -1,
      "REWARD_THEOREM_PROCEEDED": 1,
      "REWARD_PROOF_COMPLETE": 100
    },
    "higher_reward_scheme": {
      "REWARD_NULL": 0,
      "REWARD_INPUT_INVALID": -1,
      "REWARD_ASSUMPTION_INVALID": -1,
      "REWARD_DUPLICATED_RESULTS": -1,
      "REWARD_THEOREM_PROCEEDED": 1,
      "REWARD_PROOF_COMPLETE": 1000
    },
    "imitation_reward": 100
  },
  "recur": {
    "HINGE_MAX": 10,
    "HINGE_MIN": -10,
    "ent_maxsize": 20,
    "gt_maxsize": 50,
    "lemma_maxsize": 30,
    "lemma_embedding_size": 128,
    "lemma_operand_size": 5,
    "objective_maxsize": 1,
    "MODEL_PATH": "../pt_models/logic_recur_nn/checkpoint.pt",
    "DUMP_PATH": "../pt_models/logic_recur_nn/"
  }
}